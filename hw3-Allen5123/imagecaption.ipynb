{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/allen/anaconda3/envs/dlcv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import clip\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from tokenizers import Tokenizer\n",
    "import copy\n",
    "from typing import Optional, Any, Union, Callable\n",
    "from torch import Tensor\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cuda\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"device\":\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"trainpath\":\"/data/dlcv/hw3/hw3_data/p2_data/images/train/\",\n",
    "    \"trainjson\":\"/data/dlcv/hw3/hw3_data/p2_data/train.json\",\n",
    "    \"testpath\":\"/data/dlcv/hw3/hw3_data/p2_data/images/val/\",\n",
    "    \"testjson\":\"/data/dlcv/hw3/hw3_data/p2_data/val.json\",\n",
    "    \"captiontokenpath\":\"/data/dlcv/hw3/hw3_data/caption_tokenizer.json\",\n",
    "    \"model_pth\":\"/data/allen/hw3model/\",\n",
    "    \"maxcaptiontokenlen\":60,\n",
    "    \"d_model\":768,\n",
    "    \"dim_feedforward\":2048,\n",
    "    \"num_decoder_layers\":6,\n",
    "    \"trainbsz\":16,\n",
    "    \"testbsz\":256,\n",
    "    \"epochs\":20,\n",
    "    \"lr\":1.e-4\n",
    "}\n",
    "if config[\"device\"] == \"cuda\":\n",
    "    torch.cuda.set_device(2)\n",
    "print('Device used :', config[\"device\"])\n",
    "tokenizer = Tokenizer.from_file(config[\"captiontokenpath\"])\n",
    "config[\"numoftoken\"] = tokenizer.get_vocab_size()\n",
    "config[\"bos_id\"] = tokenizer.token_to_id('[BOS]')\n",
    "config[\"eos_id\"] = tokenizer.token_to_id('[EOS]')\n",
    "config[\"pad_id\"] = tokenizer.token_to_id('[PAD]')\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_seeds(seed):\n",
    "    # Python built-in random module\n",
    "    random.seed(seed)\n",
    "    # Numpy\n",
    "    np.random.seed(seed)\n",
    "    # Torch\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "same_seeds(7777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = (img + 1) / 2\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "    \n",
    "def save_checkpoint(checkpoint_path, model, optimizer):\n",
    "    state = {'model_state_dict': model.state_dict(),\n",
    "             'optimizer_state_dict' : optimizer.state_dict()}\n",
    "    torch.save(state, checkpoint_path)\n",
    "    print('model saved to {}'.format(checkpoint_path))\n",
    "\n",
    "def load_checkpoint(checkpoint_path, device='cpu'):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=torch.device(device))\n",
    "    return checkpoint[\"model_state_dict\"], checkpoint[\"optimizer_state_dict\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DS(Dataset):\n",
    "    def __init__(self, imgpath, jsonpath=None, transform=None) -> None:\n",
    "        self.data = [] #(imgpath, imgname, list of caption/None)\n",
    "        self.id2caption = {}\n",
    "        self.image2id = {}\n",
    "        self.transform = transform\n",
    "        if jsonpath is not None:\n",
    "            if os.path.exists(jsonpath):\n",
    "                with open(jsonpath) as f:\n",
    "                    jsfile = json.load(f)\n",
    "                for annotation in jsfile[\"annotations\"]:\n",
    "                    if annotation[\"image_id\"] not in self.id2caption:\n",
    "                        self.id2caption[annotation[\"image_id\"]] = []\n",
    "                    self.id2caption[annotation[\"image_id\"]].append(annotation[\"caption\"])\n",
    "                for img in jsfile[\"images\"]:\n",
    "                    self.image2id[img[\"file_name\"]] = img[\"id\"]\n",
    "            else:\n",
    "                print(\"Can't open {}\".format(jsonpath))\n",
    "                exit(-1)\n",
    "        if os.path.exists(imgpath):\n",
    "            paths = glob.glob(os.path.join(imgpath, \"*.jpg\"))\n",
    "            for path in paths:\n",
    "                imgname = os.path.split(path)[-1]\n",
    "                if jsonpath is not None:\n",
    "                    captions = self.id2caption[self.image2id[imgname]]\n",
    "                    tgt_ys, tgts = [], []\n",
    "                    for caption in captions:\n",
    "                        all_token = tokenizer.encode(caption)\n",
    "                        token_len = len(all_token.ids)\n",
    "                        token_cutbos = torch.tensor(all_token.ids[1:] + [config[\"pad_id\"]] * (config[\"maxcaptiontokenlen\"] - (token_len - 1)))\n",
    "                        token_cuteos = torch.tensor(all_token.ids[:-1] + [config[\"pad_id\"]] * (config[\"maxcaptiontokenlen\"] - (token_len - 1)))\n",
    "                        tgt_ys.append(token_cutbos), tgts.append(token_cuteos)\n",
    "                else:\n",
    "                    captions = tgt_ys = tgts = None\n",
    "                self.data.append((path, imgname, captions, tgt_ys, tgts))\n",
    "        else:\n",
    "            print(\"Can't open {}\".format(imgpath))\n",
    "            exit(-1)\n",
    "        self.len = len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        imgpath, imgname, captions, tgt_ys, tgts = self.data[index]\n",
    "        img = Image.open(imgpath)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        rd = np.random.randint(len(captions))\n",
    "        caption = None if captions is None else captions[rd]\n",
    "        tgt_y = None if tgt_ys is None else tgt_ys[rd]\n",
    "        tgt = None if tgts is None else tgts[rd]\n",
    "        return img, imgname, caption, tgt_y, tgt\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "__, preprocess = clip.load('ViT-L/14@336px', config[\"device\"])\n",
    "train_loader = DataLoader(DS(config[\"trainpath\"], config[\"trainjson\"], preprocess), batch_size=config[\"trainbsz\"], pin_memory=True, shuffle=True, num_workers=3)\n",
    "test_loader = DataLoader(DS(config[\"testpath\"], config[\"testjson\"], preprocess), batch_size=config[\"testbsz\"], pin_memory=True, shuffle=False, num_workers=3)\n",
    "visual_loader = DataLoader(DS(imgpath=\"/data/dlcv/hw3/hw3_data/p3_data/images/\", transform=preprocess), pin_memory=True, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEncoder(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.clip_encoder, __ = clip.load('ViT-L/14@336px', config[\"device\"])\n",
    "        self.clip_encoder = self.clip_encoder.float()\n",
    "        #override vit's forward\n",
    "        vit = self.clip_encoder.visual\n",
    "        bound_method = vit_forward.__get__(vit, vit.__class__)\n",
    "        setattr(vit, 'forward', bound_method)\n",
    "        #Freeze model parameters\n",
    "        for param in self.clip_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.clip_encoder.encode_image(x)\n",
    "\n",
    "def vit_forward(self, x):\n",
    "        x = self.conv1(x)  # shape = [*, width, grid, grid]\n",
    "        x = x.reshape(x.shape[0], x.shape[1], -1)  # shape = [*, width, grid ** 2]\n",
    "        x = x.permute(0, 2, 1)  # shape = [*, grid ** 2, width]\n",
    "        x = torch.cat([self.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1)  # shape = [*, grid ** 2 + 1, width]\n",
    "        x = x + self.positional_embedding.to(x.dtype)\n",
    "        x = self.ln_pre(x)\n",
    "\n",
    "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "\n",
    "        # x = self.ln_post(x[:,1:,:])\n",
    "        x = x[:,1:,:]\n",
    "        \n",
    "        if self.proj is not None:\n",
    "            x = x @ self.proj\n",
    "        return x   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Decoder\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html#torch.nn.MultiheadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDecoder(nn.Module):\n",
    "    def __init__(self, d_model: int = 512, nhead: int = 8, num_decoder_layers: int = 6, dim_feedforward: int = 2048, dropout: float = 0.1,\n",
    "                 activation: Union[str, Callable[[Tensor], Tensor]] = F.relu,\n",
    "                 layer_norm_eps: float = 1e-5, batch_first: bool = True, norm_first: bool = False,\n",
    "                 device=None, dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout,\n",
    "                                                    activation, layer_norm_eps, batch_first, norm_first,\n",
    "                                                    **factory_kwargs)\n",
    "        decoder_norm = nn.LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm)\n",
    "        self._reset_parameters()\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.batch_first = batch_first\n",
    "        self.linear = nn.Linear(d_model, config[\"numoftoken\"])\n",
    "\n",
    "    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None,\n",
    "                memory_mask: Optional[Tensor] = None,\n",
    "                tgt_key_padding_mask: Optional[Tensor] = None, memory_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "\n",
    "        output, atten_weight = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,\n",
    "                              tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                              memory_key_padding_mask=memory_key_padding_mask)\n",
    "        return self.linear(output), atten_weight\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_square_subsequent_mask(sz: int, device='cpu') -> Tensor:\n",
    "        return torch.triu(torch.full((sz, sz), float('-inf'), device=device), diagonal=1)\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    \n",
    "class TransformerDecoder(nn.Module):\n",
    "    __constants__ = ['norm']\n",
    "\n",
    "    def __init__(self, decoder_layer, num_layers, norm=None):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.layers = _get_clones(decoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "\n",
    "    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None,\n",
    "                memory_mask: Optional[Tensor] = None, tgt_key_padding_mask: Optional[Tensor] = None,\n",
    "                memory_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        output = tgt\n",
    "        for mod in self.layers:\n",
    "            output, atten_weight = mod(output, memory, tgt_mask=tgt_mask,\n",
    "                         memory_mask=memory_mask,\n",
    "                         tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                         memory_key_padding_mask=memory_key_padding_mask)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output, atten_weight\n",
    "\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    __constants__ = ['batch_first', 'norm_first']\n",
    "\n",
    "    def __init__(self, d_model: int, nhead: int, dim_feedforward: int = 2048, dropout: float = 0.1,\n",
    "                 activation: Union[str, Callable[[Tensor], Tensor]] = F.relu,\n",
    "                 layer_norm_eps: float = 1e-5, batch_first: bool = False, norm_first: bool = False,\n",
    "                 device=None, dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,\n",
    "                                            **factory_kwargs)\n",
    "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,\n",
    "                                                 **factory_kwargs)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward, **factory_kwargs)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model, **factory_kwargs)\n",
    "\n",
    "        self.norm_first = norm_first\n",
    "        self.norm1 = nn.LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.norm2 = nn.LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.norm3 = nn.LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "        # Legacy string support for activation function.\n",
    "        if isinstance(activation, str):\n",
    "            self.activation = _get_activation_fn(activation)\n",
    "        else:\n",
    "            self.activation = activation\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        if 'activation' not in state:\n",
    "            state['activation'] = F.relu\n",
    "        super(TransformerDecoderLayer, self).__setstate__(state)\n",
    "\n",
    "    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None, memory_mask: Optional[Tensor] = None,\n",
    "                tgt_key_padding_mask: Optional[Tensor] = None, memory_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        x = tgt\n",
    "        if self.norm_first:\n",
    "            x = x + self._sa_block(self.norm1(x), tgt_mask, tgt_key_padding_mask)\n",
    "            att_res, att_weight = self._mha_block(self.norm2(x), memory, memory_mask, memory_key_padding_mask)\n",
    "            x = x + att_res\n",
    "            x = x + self._ff_block(self.norm3(x))\n",
    "        else:\n",
    "            x = self.norm1(x + self._sa_block(x, tgt_mask, tgt_key_padding_mask))\n",
    "            att_res, att_weight = self._mha_block(x, memory, memory_mask, memory_key_padding_mask)\n",
    "            x = self.norm2(x + att_res)\n",
    "            x = self.norm3(x + self._ff_block(x))\n",
    "\n",
    "        return x, att_weight\n",
    "\n",
    "    # self-attention block\n",
    "    def _sa_block(self, x: Tensor,\n",
    "                  attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]) -> Tensor:\n",
    "        x = self.self_attn(x, x, x,\n",
    "                           attn_mask=attn_mask,\n",
    "                           key_padding_mask=key_padding_mask,\n",
    "                           need_weights=False)[0]\n",
    "        return self.dropout1(x)\n",
    "\n",
    "    # multihead attention block\n",
    "    def _mha_block(self, x: Tensor, mem: Tensor,\n",
    "                   attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]) -> Tensor:\n",
    "        x, att_weight = self.multihead_attn(x, mem, mem,\n",
    "                                attn_mask=attn_mask,\n",
    "                                key_padding_mask=key_padding_mask,\n",
    "                                need_weights=True)\n",
    "        return self.dropout2(x), att_weight\n",
    "\n",
    "    # feed forward block\n",
    "    def _ff_block(self, x: Tensor) -> Tensor:\n",
    "        x = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
    "        return self.dropout3(x)\n",
    "\n",
    "def _get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "def _get_activation_fn(activation: str) -> Callable[[Tensor], Tensor]:\n",
    "    if activation == \"relu\":\n",
    "        return F.relu\n",
    "    elif activation == \"gelu\":\n",
    "        return F.gelu\n",
    "\n",
    "    raise RuntimeError(\"activation should be relu/gelu, not {}\".format(activation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        # 初始化Shape为(max_len, d_model)的PE (positional encoding)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        # 初始化一个tensor [[0, 1, 2, 3, ...]]\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        # 这里就是sin和cos括号中的内容，通过e和ln进行了变换\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        # 计算PE(pos, 2i)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # 计算PE(pos, 2i+1)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        # 为了方便计算，在最外面在unsqueeze出一个batch\n",
    "        pe = pe.unsqueeze(0)\n",
    "        # 如果一个参数不参与梯度下降，但又希望保存model的时候将其保存下来\n",
    "        # 这个时候就可以用register_buffer\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x 为embedding后的inputs，例如(1,7, 128)，batch size为1,7个单词，单词维度为128\n",
    "        \"\"\"\n",
    "        # 将x和positional encoding相加。\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTransformer(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = MyEncoder()\n",
    "        self.word_embedding = nn.Embedding(config[\"numoftoken\"], config[\"d_model\"])\n",
    "        self.position_encoding = PositionalEncoding(d_model=config[\"d_model\"], dropout=0)\n",
    "        self.decoder = MyDecoder(d_model=config[\"d_model\"], num_decoder_layers=config[\"num_decoder_layers\"], dim_feedforward=config[\"dim_feedforward\"])\n",
    "\n",
    "    def forward(self, src, tgt, tgt_mask=None, tgt_key_padding_mask=None):\n",
    "        encode_features = self.encoder(src) #(bsz, #patch, d_model)\n",
    "        # print(\"encode_features {}\".format(encode_features.shape))\n",
    "        after_emb = self.word_embedding(tgt) #(bsz, maxcaptiontokenlen, d_model)\n",
    "        # print(\"afer_emb {}\".format(after_emb.shape))\n",
    "        after_enc = self.position_encoding(after_emb) #(bsz, maxcaptiontokenlen, d_model)\n",
    "        # print(\"after_enc {}\".format(after_enc.shape))\n",
    "        logit, atten_weight = self.decoder(after_enc, encode_features, tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "        # print(atten_weight.shape)\n",
    "        return logit, atten_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(pred, ans, vocab_size=config[\"numoftoken\"], label_smoothing=0.1, pad=config[\"pad_id\"]):\n",
    "    # took this \"normalizing\" from tensor2tensor. We subtract it for\n",
    "    # readability. This makes no difference on learning.\n",
    "    confidence = 1.0 - label_smoothing\n",
    "    low_confidence = (1.0 - confidence) / float(vocab_size - 1)\n",
    "    normalizing = -(\n",
    "        confidence * math.log(confidence) + float(vocab_size - 1) *\n",
    "        low_confidence * math.log(low_confidence + 1e-20))\n",
    "\n",
    "    one_hot = torch.zeros_like(pred).scatter_(1, ans.unsqueeze(1), 1)\n",
    "    one_hot = one_hot * confidence + (1 - one_hot) * low_confidence\n",
    "    log_pred = F.log_softmax(pred, dim=1)\n",
    "    #print(\"=========test2=========\", log_pred)\n",
    "\n",
    "    xent = -(one_hot * log_pred).sum(dim=1)\n",
    "    xent = xent.masked_select(ans != pad)\n",
    "    loss = (xent - normalizing).mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# seq2seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class seq2seq(nn.Module):\n",
    "    def __init__(self, modelpath=None) -> None:\n",
    "        super().__init__()\n",
    "        self.transformer = MyTransformer()\n",
    "        self.opt = optim.Adam(self.transformer.parameters(), lr=config[\"lr\"])\n",
    "        self.scheduler = optim.lr_scheduler.MultiStepLR(self.opt, milestones=[8,14], gamma=0.9)\n",
    "        if modelpath is not None:\n",
    "            model_stat, opt_stat = load_checkpoint(modelpath)\n",
    "            self.transformer.load_state_dict(model_stat)\n",
    "            self.opt.load_state_dict(opt_stat)\n",
    "            \n",
    "    def train(self):\n",
    "        tgt_mask = self.get_tgt_mask(config[\"maxcaptiontokenlen\"]).to(config[\"device\"])\n",
    "        # criterion = nn.CrossEntropyLoss(ignore_index=config[\"pad_id\"], label_smoothing=0.1)\n",
    "        criterion = get_loss\n",
    "        bestloss = torch.inf\n",
    "        for ep in range(config[\"epochs\"]):\n",
    "            self.transformer.train()\n",
    "            trainloss, valloss = 0., 0.\n",
    "            for train_idx, (img, imgname, caption, tgt_y, tgt) in enumerate(train_loader):\n",
    "                img, tgt_y, tgt = img.to(config[\"device\"]), tgt_y.to(config[\"device\"]), tgt.to(config[\"device\"])\n",
    "                tgt_key_padding_mask = self.get_key_padding_mask(tgt).to(config[\"device\"])\n",
    "                # print(\"tgt_y {} tgt {} tgt_mask {} tgt_key_mask {}\".format(tgt_y.shape, tgt.shape, tgt_mask.shape, tgt_key_padding_mask.shape))\n",
    "                logit, atten_weight = self.transformer(img, tgt, tgt_mask, tgt_key_padding_mask)\n",
    "                # print(\"logit {}\".format(logit.shape))\n",
    "                loss = criterion(logit.contiguous().view(-1, logit.size(-1)), tgt_y.contiguous().view(-1))\n",
    "                trainloss += loss.item()\n",
    "                self.opt.zero_grad()\n",
    "                loss.backward()\n",
    "                self.opt.step()\n",
    "            self.scheduler.step()\n",
    "            self.transformer.eval()\n",
    "            with torch.no_grad():\n",
    "                for val_idx, (img, imgname, caption, tgt_y, tgt) in enumerate(test_loader):\n",
    "                    img, tgt_y, tgt = img.to(config[\"device\"]), tgt_y.to(config[\"device\"]), tgt.to(config[\"device\"])\n",
    "                    tgt_key_padding_mask = self.get_key_padding_mask(tgt).to(config[\"device\"])\n",
    "                    logit, atten_weight = self.transformer(img, tgt, tgt_mask, tgt_key_padding_mask)\n",
    "                    loss = criterion(logit.contiguous().view(-1, logit.size(-1)), tgt_y.contiguous().view(-1))\n",
    "                    valloss += loss.item()\n",
    "            if valloss < bestloss:\n",
    "                self.save(ep + 1)\n",
    "                bestloss = valloss\n",
    "            print(\"Epoch[{}/{}] train_loss : {:.5f} val_loss : {:.5f}\".format(ep + 1, config[\"epochs\"], trainloss / train_idx, valloss / val_idx))\n",
    "\n",
    "    def save(self, ep):\n",
    "        savepath = os.path.join(config[\"model_pth\"], \"hw3_2_l.pth\")\n",
    "        save_checkpoint(savepath, self.transformer, self.opt)\n",
    "        print(\"Save model ---> {}\".format(savepath))\n",
    "\n",
    "    def att_vis(self):\n",
    "        model = MyTransformer().to(config[\"device\"])\n",
    "        modelckp, __ = load_checkpoint(\"/data/allen/hw3model/hw3_2_l.pth\", device=config[\"device\"])\n",
    "        model.load_state_dict(modelckp)\n",
    "        model.eval()\n",
    "        predict_caption = {}\n",
    "        with torch.no_grad():\n",
    "            tgt_mask = self.get_tgt_mask(config[\"maxcaptiontokenlen\"]).to(config[\"device\"])\n",
    "            for idx, (img, imgname, caption, __, __) in enumerate(visual_loader):\n",
    "                img = img.to(config[\"device\"])\n",
    "                tgt = torch.ones((img.shape[0], config[\"maxcaptiontokenlen\"]), dtype= torch.int32, device=config[\"device\"]) * config[\"pad_id\"]\n",
    "                tgt[:,0] = config[\"bos_id\"]\n",
    "                alleos = torch.ones((img.shape[0], ), dtype= torch.int8, device=config[\"device\"])\n",
    "                for i in range(1, config[\"maxcaptiontokenlen\"]):\n",
    "                    # print(\"{}: {}\".format(i, tgt))\n",
    "                    tgt_key_padding_mask = self.get_key_padding_mask(tgt).to(config[\"device\"])\n",
    "                    logit, atten_weight = model(img, tgt, tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "                    predict = logit.argmax(dim=-1)\n",
    "                    tgt[:,i] = predict[:,i-1]\n",
    "                    #short cut\n",
    "                    alleos[predict[:,i-1] == config[\"eos_id\"]] = 0\n",
    "                    # print(predict[:,i], alleos)\n",
    "                    if torch.sum(alleos) == 0:\n",
    "                        break\n",
    "                tgt = tgt.cpu().tolist()\n",
    "                # print(tgt)\n",
    "                for i, name in enumerate(imgname):\n",
    "                    try:\n",
    "                        first_eos = tgt[i].index(config[\"eos_id\"])\n",
    "                    except ValueError:\n",
    "                        first_eos = config[\"maxcaptiontokenlen\"]\n",
    "                    predict_caption[name] = tokenizer.decode(tgt[i][:first_eos])\n",
    "                # print(predict_caption)\n",
    "                print(\"{}/{}\".format(idx + 1, round(len(test_loader.dataset) / config[\"testbsz\"])))\n",
    "\n",
    "    def inference(self):\n",
    "        model = MyTransformer().to(config[\"device\"])\n",
    "        modelckp, __ = load_checkpoint(\"/data/allen/hw3model/hw3_2_l.pth\", device=config[\"device\"])\n",
    "        model.load_state_dict(modelckp)\n",
    "        model.eval()\n",
    "        predict_caption = {}\n",
    "        with torch.no_grad():\n",
    "            tgt_mask = self.get_tgt_mask(config[\"maxcaptiontokenlen\"]).to(config[\"device\"])\n",
    "            for idx, (img, imgname, caption, __, __) in enumerate(test_loader):\n",
    "                img = img.to(config[\"device\"])\n",
    "                tgt = torch.ones((img.shape[0], config[\"maxcaptiontokenlen\"]), dtype= torch.int32, device=config[\"device\"]) * config[\"pad_id\"]\n",
    "                tgt[:,0] = config[\"bos_id\"]\n",
    "                alleos = torch.ones((img.shape[0], ), dtype= torch.int8, device=config[\"device\"])\n",
    "                for i in range(1, config[\"maxcaptiontokenlen\"]):\n",
    "                    # print(\"{}: {}\".format(i, tgt))\n",
    "                    tgt_key_padding_mask = self.get_key_padding_mask(tgt).to(config[\"device\"])\n",
    "                    logit, atten_weight = model(img, tgt, tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "                    predict = logit.argmax(dim=-1)\n",
    "                    tgt[:,i] = predict[:,i-1]\n",
    "                    #short cut\n",
    "                    alleos[predict[:,i-1] == config[\"eos_id\"]] = 0\n",
    "                    # print(predict[:,i], alleos)\n",
    "                    if torch.sum(alleos) == 0:\n",
    "                        break\n",
    "                tgt = tgt.cpu().tolist()\n",
    "                # print(tgt)\n",
    "                for i, name in enumerate(imgname):\n",
    "                    try:\n",
    "                        first_eos = tgt[i].index(config[\"eos_id\"])\n",
    "                    except ValueError:\n",
    "                        first_eos = config[\"maxcaptiontokenlen\"]\n",
    "                    predict_caption[name.split('.')[0]] = tokenizer.decode(tgt[i][:first_eos])\n",
    "                print(\"{}/{}\".format(idx + 1, round(len(test_loader.dataset) / config[\"testbsz\"])))\n",
    "\n",
    "        with open(\"predict_l.json\", 'w') as f:\n",
    "            json.dump(predict_caption, f)\n",
    "\n",
    "    def get_tgt_mask(self, sz):\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(sz)\n",
    "        return tgt_mask < 0\n",
    "\n",
    "    def get_key_padding_mask(self, tokens):\n",
    "        key_padding_mask = torch.zeros(tokens.size())\n",
    "        key_padding_mask[tokens == config[\"pad_id\"]] = -torch.inf\n",
    "        return key_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = seq2seq()\n",
    "model = model.to(config[\"device\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/7\n",
      "2/7\n",
      "3/7\n",
      "4/7\n",
      "5/7\n",
      "6/7\n",
      "7/7\n"
     ]
    }
   ],
   "source": [
    "# model.train()\n",
    "model.inference()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('dlcv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b129b87ef853288e118a1f4c2954e4d8b1d47adc530c957c0432333233c73696"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
