{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/allen/anaconda3/envs/dlcv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import math\n",
    "import sys\n",
    "import random\n",
    "from byol_pytorch import BYOL\n",
    "from torchvision import models\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cuda\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'device':'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'train_pth':'/data/dlcv/hw4/office/train/',\n",
    "    'val_pth':'/data/dlcv/hw4/office/val/',\n",
    "    'train_csv_pth':'/data/dlcv/hw4/office/train.csv',\n",
    "    'val_csv_pth':'/data/dlcv/hw4/office/val.csv',\n",
    "    'save_pth':'/data/allen/hw4model/setting_a.pth',\n",
    "    'bsz':32,\n",
    "    'lr':1.e-3,\n",
    "    'epochs':100,\n",
    "    'imgsz':128,\n",
    "    'numofclass':65\n",
    "}\n",
    "backbone_transform = transforms.Compose([\n",
    "    transforms.Resize((config['imgsz'], config['imgsz'])),\n",
    "    transforms.CenterCrop(config['imgsz']),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=torch.tensor([0.485, 0.456, 0.406]), std=torch.tensor([0.229, 0.224, 0.225]))\n",
    "])\n",
    "if config[\"device\"] == \"cuda\":\n",
    "    torch.cuda.set_device(5)\n",
    "print('Device used :', config['device'])\n",
    "label2class = {'Alarm_Clock': 0, 'Backpack': 1, 'Batteries': 2, 'Bed': 3, 'Bike': 4, 'Bottle': 5, 'Bucket': 6, 'Calculator': 7, 'Calendar': 8, 'Candles': 9, 'Chair': 10, 'Clipboards': 11, 'Computer': 12, \n",
    "    'Couch': 13, 'Curtains': 14, 'Desk_Lamp': 15, 'Drill': 16, 'Eraser': 17, 'Exit_Sign': 18, 'Fan': 19, 'File_Cabinet': 20, 'Flipflops': 21, 'Flowers': 22, 'Folder': 23, 'Fork': 24, 'Glasses': 25,\n",
    "    'Hammer': 26, 'Helmet': 27, 'Kettle': 28, 'Keyboard': 29, 'Knives': 30, 'Lamp_Shade': 31, 'Laptop': 32, 'Marker': 33, 'Monitor': 34, 'Mop': 35, 'Mouse': 36, 'Mug': 37, 'Notebook': 38,\n",
    "    'Oven': 39, 'Pan': 40, 'Paper_Clip': 41, 'Pen': 42, 'Pencil': 43, 'Postit_Notes': 44, 'Printer': 45, 'Push_Pin': 46, 'Radio': 47, 'Refrigerator': 48, 'Ruler': 49, 'Scissors': 50, 'Screwdriver': 51,\n",
    "    'Shelf': 52, 'Sink': 53, 'Sneakers': 54, 'Soda': 55, 'Speaker': 56, 'Spoon': 57, 'TV': 58, 'Table': 59, 'Telephone': 60, 'ToothBrush': 61, 'Toys': 62, 'Trash_Can': 63, 'Webcam': 64}\n",
    "class2label = {0: 'Alarm_Clock', 1: 'Backpack', 2: 'Batteries', 3: 'Bed', 4: 'Bike', 5: 'Bottle', 6: 'Bucket', 7: 'Calculator', 8: 'Calendar', 9: 'Candles', 10: 'Chair', 11: 'Clipboards', 12: 'Computer', \n",
    "    13: 'Couch', 14: 'Curtains', 15: 'Desk_Lamp', 16: 'Drill', 17: 'Eraser', 18: 'Exit_Sign', 19: 'Fan', 20: 'File_Cabinet', 21: 'Flipflops', 22: 'Flowers', 23: 'Folder', 24: 'Fork', 25: 'Glasses', \n",
    "    26: 'Hammer', 27: 'Helmet', 28: 'Kettle', 29: 'Keyboard', 30: 'Knives', 31: 'Lamp_Shade', 32: 'Laptop', 33: 'Marker', 34: 'Monitor', 35: 'Mop', 36: 'Mouse', 37: 'Mug', 38: 'Notebook', \n",
    "    39: 'Oven', 40: 'Pan', 41: 'Paper_Clip', 42: 'Pen', 43: 'Pencil', 44: 'Postit_Notes', 45: 'Printer', 46: 'Push_Pin', 47: 'Radio', 48: 'Refrigerator', 49: 'Ruler', 50: 'Scissors', 51: 'Screwdriver', \n",
    "    52: 'Shelf', 53: 'Sink', 54: 'Sneakers', 55: 'Soda', 56: 'Speaker', 57: 'Spoon', 58: 'TV', 59: 'Table', 60: 'Telephone', 61: 'ToothBrush', 62: 'Toys', 63: 'Trash_Can', 64: 'Webcam'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(checkpoint_path, model, optimizer):\n",
    "    state = {'model_state_dict': model.state_dict(),\n",
    "             'optimizer_state_dict' : optimizer.state_dict()}\n",
    "    torch.save(state, checkpoint_path)\n",
    "    print('model saved to {}'.format(checkpoint_path))\n",
    "    \n",
    "def load_checkpoint(checkpoint_path, device='cpu'):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    return checkpoint[\"model_state_dict\"], checkpoint[\"optimizer_state_dict\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DS(Dataset):\n",
    "    def __init__(self, datapath, csvpath, transform=None) -> None:\n",
    "        self.transform = transform\n",
    "        self.data = [] #(imgpath, imgname, #label)\n",
    "        if csvpath is not None:\n",
    "            if os.path.exists(csvpath):\n",
    "                df = pd.read_csv(csvpath)\n",
    "                self.data = [(os.path.join(datapath, name), name, label2class[label]) for name, label in zip(df['filename'], df['label'])]\n",
    "            else:\n",
    "                print(f\"Can't find {csvpath}\")\n",
    "                exit(-1)\n",
    "        else:\n",
    "            if os.path.exists(datapath):\n",
    "                paths = glob.glob(os.path.join(datapath, \"*\"))\n",
    "                for path in paths:\n",
    "                    imgname = os.path.split(path)[-1]\n",
    "                    self.data.append((path, imgname, None))\n",
    "            else:\n",
    "                print(f\"Can't open {datapath}\")\n",
    "                exit(-1)\n",
    "        self.len = len(self.data)\n",
    "        print(self.len)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        imgpath, imgname, label = self.data[index]\n",
    "        img = Image.open(imgpath)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label if label is not None else img\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3951\n",
      "406\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(DS(config['train_pth'], config['train_csv_pth'], transform=backbone_transform), batch_size=config['bsz'], shuffle=True, pin_memory=True)\n",
    "val_loader = DataLoader(DS(config['val_pth'], config['val_csv_pth'], transform=backbone_transform), batch_size=config['bsz'], pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildLabelDict(train_csv):\n",
    "    label2class, class2label = {}, {}\n",
    "    train_df = pd.read_csv(train_csv)\n",
    "    labellist = sorted(list(dict.fromkeys([name for name in train_df['label']])))\n",
    "    label2class = {label : idx for (idx, label) in enumerate(labellist)}\n",
    "    class2label = {idx : label for (idx, label) in enumerate(labellist)}\n",
    "    print(label2class, class2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierA(nn.Module):\n",
    "    def __init__(self, backbone_pth=None) -> None:\n",
    "        super().__init__()\n",
    "        self.backbone = models.resnet50(weights=None)\n",
    "        self.backbone.fc = nn.Linear(2048, config['numofclass'])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassifierA(\n",
      "  (backbone): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Linear(in_features=2048, out_features=65, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = ClassifierA().to(config['device'])\n",
    "opt = torch.optim.Adam(model.parameters(), lr=config['lr'])\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=config['epochs'])\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved to /data/allen/hw4model/setting_a.pth\n",
      "Epoch [1/100] train_loss : 4.520236540616043 train_acc : 4.25% val_loss : 6.559268613656362 val_acc : 6.16%\n",
      "model saved to /data/allen/hw4model/setting_a.pth\n",
      "Epoch [2/100] train_loss : 4.014544576164184 train_acc : 6.40% val_loss : 4.225553035736084 val_acc : 7.14%\n",
      "Epoch [3/100] train_loss : 3.8990919977668823 train_acc : 8.02% val_loss : 4.23883597056071 val_acc : 7.14%\n",
      "model saved to /data/allen/hw4model/setting_a.pth\n",
      "Epoch [4/100] train_loss : 3.8321592865920646 train_acc : 9.36% val_loss : 4.488479673862457 val_acc : 11.08%\n",
      "Epoch [5/100] train_loss : 3.7840810868798234 train_acc : 10.17% val_loss : 4.353418409824371 val_acc : 10.10%\n",
      "Epoch [6/100] train_loss : 3.685094282879093 train_acc : 12.15% val_loss : 4.887636800607045 val_acc : 10.10%\n",
      "model saved to /data/allen/hw4model/setting_a.pth\n",
      "Epoch [7/100] train_loss : 3.5832484892713343 train_acc : 13.06% val_loss : 3.7691844701766968 val_acc : 14.53%\n",
      "model saved to /data/allen/hw4model/setting_a.pth\n",
      "Epoch [8/100] train_loss : 3.4300417027822356 train_acc : 14.81% val_loss : 3.803279221057892 val_acc : 17.24%\n",
      "model saved to /data/allen/hw4model/setting_a.pth\n",
      "Epoch [9/100] train_loss : 3.2964972480525816 train_acc : 19.41% val_loss : 3.7610042889912925 val_acc : 17.73%\n",
      "model saved to /data/allen/hw4model/setting_a.pth\n",
      "Epoch [10/100] train_loss : 3.246962950481632 train_acc : 19.49% val_loss : 3.6005889972050986 val_acc : 19.70%\n",
      "Epoch [11/100] train_loss : 3.0908268777335564 train_acc : 22.80% val_loss : 3.6805628736813865 val_acc : 18.72%\n",
      "model saved to /data/allen/hw4model/setting_a.pth\n",
      "Epoch [12/100] train_loss : 2.931725257780494 train_acc : 25.84% val_loss : 3.3567495147387185 val_acc : 22.17%\n",
      "Epoch [13/100] train_loss : 2.8933837956529325 train_acc : 26.63% val_loss : 4.054699858029683 val_acc : 21.67%\n",
      "model saved to /data/allen/hw4model/setting_a.pth\n",
      "Epoch [14/100] train_loss : 2.7371907951385994 train_acc : 30.17% val_loss : 3.3179218570391336 val_acc : 22.66%\n",
      "model saved to /data/allen/hw4model/setting_a.pth\n",
      "Epoch [15/100] train_loss : 2.544964738977634 train_acc : 33.43% val_loss : 3.4865762193997702 val_acc : 22.91%\n",
      "Epoch [16/100] train_loss : 2.3637694837601204 train_acc : 38.47% val_loss : 3.852032721042633 val_acc : 21.43%\n",
      "model saved to /data/allen/hw4model/setting_a.pth\n",
      "Epoch [17/100] train_loss : 2.1756918963378036 train_acc : 41.53% val_loss : 3.5727609395980835 val_acc : 23.65%\n",
      "model saved to /data/allen/hw4model/setting_a.pth\n",
      "Epoch [18/100] train_loss : 2.0182611486776088 train_acc : 44.67% val_loss : 3.4910775820414224 val_acc : 27.83%\n",
      "model saved to /data/allen/hw4model/setting_a.pth\n",
      "Epoch [19/100] train_loss : 1.7025542622659264 train_acc : 52.95% val_loss : 3.6140377124150596 val_acc : 28.08%\n",
      "Epoch [20/100] train_loss : 1.4364857033985416 train_acc : 58.42% val_loss : 3.849497934182485 val_acc : 27.34%\n",
      "Epoch [21/100] train_loss : 1.1881750250250342 train_acc : 66.08% val_loss : 4.088052014509837 val_acc : 26.85%\n",
      "model saved to /data/allen/hw4model/setting_a.pth\n",
      "Epoch [22/100] train_loss : 0.9802870168918516 train_acc : 71.25% val_loss : 4.293706337610881 val_acc : 28.82%\n",
      "Epoch [23/100] train_loss : 0.7742993768637743 train_acc : 77.17% val_loss : 4.523661653200786 val_acc : 27.83%\n",
      "model saved to /data/allen/hw4model/setting_a.pth\n",
      "Epoch [24/100] train_loss : 0.528233762073323 train_acc : 83.93% val_loss : 4.565762599309285 val_acc : 30.30%\n",
      "Epoch [25/100] train_loss : 0.408856091945152 train_acc : 87.80% val_loss : 4.719223697980245 val_acc : 26.11%\n",
      "Epoch [26/100] train_loss : 0.3131763588606827 train_acc : 90.74% val_loss : 5.256448209285736 val_acc : 24.14%\n",
      "Epoch [27/100] train_loss : 0.2528962916959592 train_acc : 92.69% val_loss : 5.357261021931966 val_acc : 26.11%\n",
      "Epoch [28/100] train_loss : 0.2324995931268223 train_acc : 93.37% val_loss : 5.304162045319875 val_acc : 28.57%\n",
      "Epoch [29/100] train_loss : 0.17869276536734607 train_acc : 95.09% val_loss : 5.5550355315208435 val_acc : 28.33%\n",
      "Epoch [30/100] train_loss : 0.17016174439795134 train_acc : 95.09% val_loss : 5.945528666178386 val_acc : 25.37%\n",
      "Epoch [31/100] train_loss : 0.13073262636648203 train_acc : 96.58% val_loss : 5.4475182096163435 val_acc : 26.60%\n",
      "Epoch [32/100] train_loss : 0.1348444117324018 train_acc : 96.00% val_loss : 5.449312130610148 val_acc : 28.08%\n",
      "Epoch [33/100] train_loss : 0.0905047147496929 train_acc : 97.70% val_loss : 5.9971997539202375 val_acc : 27.59%\n",
      "Epoch [34/100] train_loss : 0.11711169539276904 train_acc : 96.79% val_loss : 5.942243278026581 val_acc : 27.09%\n",
      "Epoch [35/100] train_loss : 0.08053577561650758 train_acc : 98.08% val_loss : 5.53303994735082 val_acc : 28.82%\n",
      "Epoch [36/100] train_loss : 0.08239543733065872 train_acc : 97.85% val_loss : 5.65242741505305 val_acc : 30.30%\n",
      "Epoch [37/100] train_loss : 0.05542481871798816 train_acc : 98.58% val_loss : 6.255134264628093 val_acc : 28.57%\n",
      "Epoch [38/100] train_loss : 0.073388767265904 train_acc : 98.08% val_loss : 5.756850938002269 val_acc : 28.82%\n",
      "model saved to /data/allen/hw4model/setting_a.pth\n",
      "Epoch [39/100] train_loss : 0.04527738720663195 train_acc : 99.11% val_loss : 5.573942045370738 val_acc : 31.03%\n",
      "Epoch [40/100] train_loss : 0.03399206953013422 train_acc : 99.16% val_loss : 5.694544951121013 val_acc : 30.30%\n",
      "model saved to /data/allen/hw4model/setting_a.pth\n",
      "Epoch [41/100] train_loss : 0.0352475410810013 train_acc : 98.94% val_loss : 5.643526971340179 val_acc : 31.28%\n",
      "Epoch [42/100] train_loss : 0.06749428055302156 train_acc : 97.92% val_loss : 6.389703392982483 val_acc : 29.06%\n",
      "Epoch [43/100] train_loss : 0.10727019579816095 train_acc : 96.84% val_loss : 5.879097243150075 val_acc : 28.33%\n",
      "Epoch [44/100] train_loss : 0.14141930795721408 train_acc : 95.77% val_loss : 5.860195994377136 val_acc : 28.57%\n",
      "Epoch [45/100] train_loss : 0.07636661146108698 train_acc : 97.80% val_loss : 5.91081960995992 val_acc : 29.56%\n",
      "Epoch [46/100] train_loss : 0.06351511792101844 train_acc : 97.95% val_loss : 5.946784317493439 val_acc : 29.06%\n",
      "Epoch [47/100] train_loss : 0.07324113275352259 train_acc : 97.70% val_loss : 6.066296736399333 val_acc : 30.79%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m      5\u001b[0m train_loss, train_acc, val_loss, val_acc \u001b[39m=\u001b[39m \u001b[39m0.\u001b[39m, \u001b[39m0.\u001b[39m, \u001b[39m0.\u001b[39m, \u001b[39m0.\u001b[39m\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfor\u001b[39;00m idx, (img, label) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m      7\u001b[0m     img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mto(config[\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      8\u001b[0m     logit \u001b[39m=\u001b[39m model(img)\u001b[39m.\u001b[39mcpu()\n",
      "File \u001b[0;32m~/anaconda3/envs/dlcv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/dlcv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/dlcv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/dlcv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn [4], line 28\u001b[0m, in \u001b[0;36mDS.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     26\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(imgpath)\n\u001b[1;32m     27\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform:\n\u001b[0;32m---> 28\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[1;32m     29\u001b[0m \u001b[39mreturn\u001b[39;00m img, label \u001b[39mif\u001b[39;00m label \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m img\n",
      "File \u001b[0;32m~/anaconda3/envs/dlcv/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/anaconda3/envs/dlcv/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/dlcv/lib/python3.10/site-packages/torchvision/transforms/transforms.py:346\u001b[0m, in \u001b[0;36mResize.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m    339\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[39m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[39m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mresize(img, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msize, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minterpolation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mantialias)\n",
      "File \u001b[0;32m~/anaconda3/envs/dlcv/lib/python3.10/site-packages/torchvision/transforms/functional.py:474\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    472\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    473\u001b[0m     pil_interpolation \u001b[39m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[0;32m--> 474\u001b[0m     \u001b[39mreturn\u001b[39;00m F_pil\u001b[39m.\u001b[39;49mresize(img, size\u001b[39m=\u001b[39;49moutput_size, interpolation\u001b[39m=\u001b[39;49mpil_interpolation)\n\u001b[1;32m    476\u001b[0m \u001b[39mreturn\u001b[39;00m F_t\u001b[39m.\u001b[39mresize(img, size\u001b[39m=\u001b[39moutput_size, interpolation\u001b[39m=\u001b[39minterpolation\u001b[39m.\u001b[39mvalue, antialias\u001b[39m=\u001b[39mantialias)\n",
      "File \u001b[0;32m~/anaconda3/envs/dlcv/lib/python3.10/site-packages/torchvision/transforms/functional_pil.py:252\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39misinstance\u001b[39m(size, \u001b[39mlist\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(size) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m):\n\u001b[1;32m    250\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGot inappropriate size arg: \u001b[39m\u001b[39m{\u001b[39;00msize\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 252\u001b[0m \u001b[39mreturn\u001b[39;00m img\u001b[39m.\u001b[39;49mresize(\u001b[39mtuple\u001b[39;49m(size[::\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]), interpolation)\n",
      "File \u001b[0;32m~/anaconda3/envs/dlcv/lib/python3.10/site-packages/PIL/Image.py:2046\u001b[0m, in \u001b[0;36mImage.resize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2042\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mreducing_gap must be 1.0 or greater\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   2044\u001b[0m size \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(size)\n\u001b[0;32m-> 2046\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m   2047\u001b[0m \u001b[39mif\u001b[39;00m box \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2048\u001b[0m     box \u001b[39m=\u001b[39m (\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msize\n",
      "File \u001b[0;32m~/anaconda3/envs/dlcv/lib/python3.10/site-packages/PIL/ImageFile.py:257\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[1;32m    252\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mimage file is truncated \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    253\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(b)\u001b[39m}\u001b[39;00m\u001b[39m bytes not processed)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    254\u001b[0m         )\n\u001b[1;32m    256\u001b[0m b \u001b[39m=\u001b[39m b \u001b[39m+\u001b[39m s\n\u001b[0;32m--> 257\u001b[0m n, err_code \u001b[39m=\u001b[39m decoder\u001b[39m.\u001b[39;49mdecode(b)\n\u001b[1;32m    258\u001b[0m \u001b[39mif\u001b[39;00m n \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    259\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_acc = 0.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "for ep in range(config['epochs']):\n",
    "    model.train()\n",
    "    train_loss, train_acc, val_loss, val_acc = 0., 0., 0., 0.\n",
    "    for idx, (img, label) in enumerate(train_loader):\n",
    "        img = img.to(config['device'])\n",
    "        logit = model(img).cpu()\n",
    "        loss = criterion(logit, label)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        train_loss += loss.item()\n",
    "        train_acc += (logit.argmax(-1) == label).float().sum()\n",
    "    scheduler.step()\n",
    "    train_loss /= idx\n",
    "    train_acc /= len(train_loader.dataset)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (img, label) in enumerate(val_loader):\n",
    "            img = img.to(config['device'])\n",
    "            logit = model(img).cpu()\n",
    "            loss = criterion(logit, label)\n",
    "            val_loss += loss.item()\n",
    "            val_acc += (logit.argmax(-1) == label).float().sum()\n",
    "        val_loss /= idx\n",
    "        val_acc /= len(val_loader.dataset)\n",
    "\n",
    "    if val_acc > best_acc:\n",
    "        save_checkpoint(config['save_pth'], model, opt)\n",
    "        best_acc = val_acc\n",
    "    print(f\"Epoch [{ep+1}/{config['epochs']}] train_loss : {train_loss} train_acc : {train_acc:.2%} val_loss : {val_loss} val_acc : {val_acc:.2%}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('dlcv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:26:04) [GCC 10.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b129b87ef853288e118a1f4c2954e4d8b1d47adc530c957c0432333233c73696"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
